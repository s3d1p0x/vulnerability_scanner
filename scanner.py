#!/usr/bin/python3

import requests
import re
import urllib.parse
from bs4 import BeautifulSoup

class Scanner:
    def __init__(self, url, ignore_links):
        self.session = requests.Session() # Permet de créer une session pour effectuer des requêtes HTTP
        self.target_url = url # Pour stocker l'URL cible que l'on souhaite analyser
        self.target_links = [] # Permet d'initialiser une liste vide pour stocker les liens trouvés sur la page cible
        self.links_to_ignore = ignore_links # Pour stocker une liste de liens à ignorer lors de l'analyse

    def extract_links_form(self, url):
        response = self.session.get(url) # Permet d'envoyer une requête HTTP GET à l'URL fournie et stocke la réponse dans la variable response
        return re.findall('(?:href=")(.*?)"', response.content.decode(errors="ignore")) # Pour extraire tous les liens de la page.L'option errors="ignore" permet d'ignorer les caractères invalides
    
    def crawl(self, url=None):
        if url == None:
            url = self.target_url # Permet de définir l'URL de départ comme étant la cible principale
        href_links = self.extract_links_form(url) # Pour récupérer tous les liens de la page
        for link in href_links:
            link = urllib.parse.urljoin(url, link) # Permet de combiner l'URL de base et le lien relatif pour créer un lien absolu

            if "#" in link:
                link = link.split("#")[0] # Pour retirer les éventuels fragments dans l'URL qui suivent le symbole #

            if self.target_url in link and link not in self.target_links and link not in self.links_to_ignore: # Pour vérifier que le lien est dans le domaine de la cible principale, qu'il n'a pas déjà été ajouté à la liste de liens à suivre, et qu'il n'a pas été ajouté à la liste de liens à ignorer
                self.target_links.append(link) # Le lien est ajouté à la liste de liens à suivre 
                print(link)
                self.crawl(link) # Pour parcourir les sous-liens de ce lien 

    def extract_forms(self, url):
        response = self.session.get(url)
        parsed_html = BeautifulSoup(response.content) # Permet d'analyser et de manipuler facilement le contenu HTML
        return parsed_html.findAll("form") # findAll permet de rechercher tous les éléments HTML <form> de la page
    
    def submit_form(self, form, value, url):
        action = form.get("action") # Permet de récupérer l'URL de l'action du formulaire HTML
        post_url = urllib.parse.urljoin(url, action) # Pour combiner l'URL de base et l'URL de l'action pour obtenir l'URL finale à laquelle envoyer la requête
        method = form.get("method") # Permet de récupérer la méthode HTTP utilisée par le formulaire HTML
        inputs_list = form.findAll("input") # Pour récupérer tous les éléments HTML <input> du formulaire HTML, qui contiennent les données du formulaire.
        post_data = {} # Initialise un dictionnaire vide pour stocker les données du formulaire.
        for input in inputs_list:
            inputs_name = input.get("name")
            input_type = input.get("type")
            inputs_value = input.get("value")
            if input_type == "text":
                inputs_value = value # Permet de remplir automatiquement le champ de texte avec une valeur spécifiée

            post_data[inputs_name] = inputs_value # Pour ajouter la paire clé-valeur au dictionnaire post_data
        if method == "post": 
            return self.session.post(post_url, data=post_data) # Pour envoyer une requête POST
        return self.session.get(post_url, params=post_data) # Sinon, envoie une requête GET
    
    def run_scanner(self):
        for link in self.target_links:
            forms = self.extract_forms(link) # Permet de récupérer tous les formulaires HTML sur le lien cible
            for form in forms:
                print("[+] Testing form in " + link)
                is_vulnerable_to_xss = self.test_xss_in_form(form, link) # Pour tester si le formulaire est vulnérable à une attaque XSS
                if is_vulnerable_to_xss:
                    print("\n\n[***] XSS discovered in " + link + " in the following form")
                    print(form)

            if "=" in link: # Permet de vérifier si le lien contient un paramètre, ce qui peut indiquer la présence d'une vulnérabilité XSS
                print("\n\n[+] Testing " + link)
                is_vulnerable_to_xss = self.test_xss_in_link(link) # Pour tester si le formulaire est vulnérable à une attaque XSS
                if is_vulnerable_to_xss:
                    print("[***] Discovered XSS in " + link)

    def test_xss_in_link(self, url):
        xss_test_script = "<sCript>alert('test')</scriPt>" # Charge utile XSS utilisée pour le test
        url = url.replace("=", "=" + xss_test_script) # Permet d'injecter la charge utile dans le premier paramètre trouvé dans l'URL
        response = self.session.get(url) # Pour envoyer une requête GET pour accéder à l'URL modifiée
        return xss_test_script.encode() in response.content # Permet de vérifier si la charge utile de test a été exécutée

    def test_xss_in_form(self, form, url):
        xss_test_script = "<sCript>alert('test')</scriPt>" # Charge utile XSS utilisée pour le test
        response = self.submit_form(form, xss_test_script, url) # Pour envoyer une requête POST ou GET pour soumettre le formulaire avec la charge utile XSS
        return xss_test_script.encode() in response.content
